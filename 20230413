## 250bpX2

ssh smomw536@nesh-fe.rz.uni-kiel.de
cd $WORK/database
mkdir 20230413
cd 20230413
mkdir raw
sftp dneedham@download.ccga.uni-kiel.de
2598AQ4J
cd jf_needham_amplicon_1608 ~~
get *
exit
rename _23Mar \.23Mar *gz
rm  *DS28* *DS29* *DS30* 
cd ..
mkdir amplicon_reads
mv raw/*gz amplicon_reads

#fastqc
conda activate fastqc
mkdir fastqc_out
for i in `ls /gxfs_work1/geomar/smomw536/database/20230413/amplicon_reads/*.gz | head -6`
do
fastqc -o fastqc_out $i
done

#on the local terminal (/Users/ksookyoung/work/fastqc/20230413)
scp smomw536@nesh-fe.rz.uni-kiel.de:/gxfs_work1/geomar/smomw536/database/20230413/fastqc_out/*html .
#check the fastqc result

mkdir trim.amplicon_reads

#cutadapt
conda activate cutadapt
for i in `ls amplicon_reads | grep _R1_`
do
filestem=`basename $i _R1_001.fastq.gz`
R1=amplicon_reads/$filestem"_R1_001.fastq.gz"
R2=amplicon_reads/$filestem"_R2_001.fastq.gz"
sbatch -p cluster -t 4:00:00 --mem=100000 -J cut -o cut.out -e cut.err -n 6 --wrap="cutadapt -g GTGYCAGCMGCCGCGGTAA -G CCGYCAATTYMTTTRAGTTT -e 0.2 --no-indels -j 6 --discard-untrimmed -o trim.$R1 -p trim.$R2 $R1 $R2"
done
conda deactivate cutadapt

#bbsplit, data downloading is on 20221206 file
conda activate bbmap
mkdir bbsplit.seqs
for i in `ls trim.amplicon_reads/*R1*`
do
filestem=`basename $i _R1_001.fastq.gz`
R1=trim.amplicon_reads/$filestem"_R1_001.fastq.gz"
R2=trim.amplicon_reads/$filestem"_R2_001.fastq.gz"
sbatch -p cluster -t 1:00:00 --mem=100000 -J bbsplit -o bbsplit.out -e bbsplit.err -n 30 --wrap="bbsplit.sh usequality=f qtrim=f minratio=0.30 minid=0.30 pairedonly=f threads=30 -Xmx100g path=$WORK/databases/EUK-PROK-bbsplit-db in=$R1 in2=$R2 basename=bbsplit.seqs/$filestem.%_#.fastq"
done

mkdir bbsplit.seqs/Prok_reads
mv bbsplit.seqs/*SILVA_132_PROK.cdhit95p* bbsplit.seqs/Prok_reads

rename \.SILVA_132_PROK.cdhit95pc_1.fastq _R1_001.fastq bbsplit.seqs/Prok_reads/*1.fastq
rename \.SILVA_132_PROK.cdhit95pc_2.fastq _R2_001.fastq bbsplit.seqs/Prok_reads/*2.fastq

for i in bbsplit.seqs/Prok_reads/*fastq
do
sbatch -p cluster -t 10:00 --mem=100000 -J gzip -o gzip.out -e gzip.err -n 1 --wrap="gzip $i"
done
conda deactivate

#qiime2 import/denoise
conda activate qiime2-2022.2
sbatch -p cluster -t 2:00:00 --mem=100000 -J importqiime -o import.out -e import.err -n 1 --wrap="qiime tools import --type 'SampleData[PairedEndSequencesWithQuality]' --input-path bbsplit.seqs/Prok_reads/ --input-format CasavaOneEightSingleLanePerSampleDirFmt --output-path prok.reads.qza"

sbatch -p cluster -t 24:00:00 --mem=100000 -J denoise -o denoise.out -e denoise.err -n 30 --wrap="qiime dada2 denoise-paired --i-demultiplexed-seqs prok.reads.qza --p-trunc-len-f 220 --p-trunc-len-r 220 --output-dir denoise.prok.reads.qza --p-n-threads 30 --verbose"

#hey next time, run it with --verbose to learn how it works
sbatch -p cluster -t 24:00:00 --mem=100000 -J extractread -o extractread.out -e extractread.err -n 30 --wrap="qiime feature-classifier extract-reads --i-sequences /gxfs_work1/geomar/smomw536/databases/silva-138-99-seqs.qza --output-dir tax.denoise.prok.reads.qza --p-f-primer GTGYCAGCMGCCGCGGTAA --p-r-primer CCGYCAATTYMTTTRAGTTT --p-n-jobs 30 --p-read-orientation 'forward' --o-reads silva-138.1-ssu-nr99-seqs-515f-926r.qza"

sbatch -p cluster -t 24:00:00 --mem=100000 -J vsearch -o vsearch.out -e vsearch.err -n 30 --wrap="qiime feature-classifier classify-consensus-vsearch --p-threads 30 --i-query denoise.prok.reads.qza/representative_sequences.qza --i-reference-taxonomy $WORK/databases/silva-138-99-tax.qza --i-reference-reads  $WORK/databases/silva-138.1-ssu-nr99-seqs-515f-926r.qza --output-dir vsearch.tax.denoise.prok.reads.qza.trimmed"

qiime taxa barplot \
--i-table denoise.prok.reads.qza/table.qza \
--i-taxonomy vsearch.tax.denoise.prok.reads.qza.trimmed/classification.qza \
--o-visualization taxa-bar-tax.denoise.prok.reads.qzv

#on local computer
cd /Users/ksookyoung/work/qiime
scp smomw536@nesh-fe.rz.uni-kiel.de:/gxfs_work1/geomar/smomw536/database/20230413/taxa-bar-tax.denoise.prok.reads.qzv .


zgrep -c "@M50181" $WORK/database/20230413/amplicon_reads/*
zgrep -c "@A00556:330:HMCLCDRX2" /gxfs_work1/cau/sulky231/*R1*gz
for i in `ls ./amplicon_reads`
do
zcat $i
done
###########################################################################################
mkdir bbsplit.seqs/Euk_reads
mkdir bbsplit.seqs/Euk_reads/concatenate
mv bbsplit.seqs/*SILVA_132_and_PR2_EUK.cdhit95* bbsplit.seqs/Euk_reads 

cat bbsplit.seqs/Euk_reads/* |grep -o '@M50181' |wc -l
##54

for i in `ls ./bbsplit.seqs/Euk_reads`
do
cat ./bbsplit.seqs/Euk_reads/$i |grep -o '@M50181' |wc -l
done
## all less than 13 counts


#main storage
/gxfs_work1/geomar/smomw456/needham_illumina_data

conda activate qiime2-2022.2
# ? qiime tools export --input-path denoise.prok.reads.qza/table.qza --output-path denoise.prok.reads.qza.biom

qiime tools export --input-path vsearch.tax.denoise.prok.reads.qza.trimmed/classification.qza --output-path vsearch.tax.denoise.prok.reads.qza.trimmed.classification

qiime tools export --input-path denoise.prok.reads.qza/representative_sequences.qza --output-path denoise.prok.reads.qza.representative.seqs

qiime tools export --input-path vsearch.tax.denoise.prok.reads.qza.trimmed/classification.qza --output-path prok.classification
qiime tools export --input-path denoise.prok.reads.qza/table.qza --output-path denoise.prok.reads.qza.biom
sed 's/Feature ID/#OTUID/g' prok.classification/taxonomy.tsv  | sed 's/Taxon/taxonomy/g' | sed 's/Consensus/confidence/g' > tax.denoise.prok.reads.qza/reformat.taxonomy.tsv 
biom add-metadata -i denoise.prok.reads.qza.biom/feature-table.biom -o denoise.prok.reads.qza.biom/w.tax.feature-table.biom --observation-metadata-fp tax.denoise.prok.reads.qza/reformat.taxonomy.tsv --sc-separated taxonomy
biom convert -i denoise.prok.reads.qza.biom/w.tax.feature-table.biom --to-tsv -o denoise.prok.reads.qza.biom/w.tax.feature-table.biom.tsv --header-key taxonomy


